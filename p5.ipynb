{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5\n",
    "\n",
    "## Language models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Thomas Vogenthaler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment we will explore language models.\n",
    "To do so, we will encapsulate a language model using a class called `language_model` which implements language models with Laplace add-one smoothing.\n",
    "An overview of the methods your class should have is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/anaconda-2019.07/lib/python3.7/site-packages (0.24.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/anaconda-2019.07/lib/python3.7/site-packages (from pandas) (2.8.0)\r\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/anaconda-2019.07/lib/python3.7/site-packages (from pandas) (2019.1)\r\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/anaconda-2019.07/lib/python3.7/site-packages (from pandas) (1.16.4)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/anaconda-2019.07/lib/python3.7/site-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\r\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import itertools\n",
    "import re\n",
    "import decimal\n",
    "!pip install pandas\n",
    "##Bash script to import pandas\n",
    "import pandas as pd \n",
    "import glob\n",
    "from collections import defaultdict\n",
    "from math import log2\n",
    "#from string import maketrans\n",
    "class language_model:\n",
    "\n",
    "    def __init__(self, ngram=1) :\n",
    "        \"\"\"\n",
    "        Initialize a language model\n",
    "        \"\"\"\n",
    "        self.V = -1\n",
    "        ##Set V = -1 this is for our ngram usage\n",
    "        self.ngram = ngram\n",
    "        #Declare ngram within self\n",
    "        self.sentences =[]\n",
    "        #This is so we can split up into sentence\n",
    "        self.model = defaultdict(lambda: 0)\n",
    "        #This is our model that we are using\n",
    "        self.modelminus = defaultdict(lambda: 0)\n",
    "        #Model minus is set to defauldict lambda function\n",
    "        self.words = 0\n",
    "        self.ngramCount = -1\n",
    "        self.ngramE = -1\n",
    "#################################################################################################################\n",
    "    def train(self, file_name) :\n",
    "        \"\"\"\n",
    "        train a language model\n",
    "    \n",
    "        Parameters:\n",
    "        file_name is a file that contains the training set for the model\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        Open file and set text equal to the content over the file.\n",
    "        This is so we can read in the file\n",
    "        Be able to parse the data into unigram, Bigram, and trigram\n",
    "        \n",
    "        \n",
    "        After its parsed we can use sparcity and perplexity \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.V = self.unigrams(file_name)\n",
    "        self.ngramE = self.V ** self.ngram\n",
    "        ##Set the self = to the clean text\n",
    "        input_text = \"?:!-\\n\"\n",
    "        output_string = \"...  \"\n",
    "        \"\"\"Have to covert all the strings.\n",
    "        We have to remove all the unecessary characters from the text\n",
    "        To do this we have parse out all the text\"\"\"\n",
    "        remove_char = \",;\\\"(\\)\\\\\\/\"\n",
    "        begin_char= \"<s> \"\n",
    "        end_char = \" </s>\"\n",
    "        \"\"\"Remove the strings of / \\ and others\n",
    "        Get the beginning character\n",
    "        Get the end end character\n",
    "        Check for those characters \n",
    "        Next we have to convert\"\"\"\n",
    "        trans = str.maketrans(input_text ,output_string ,remove_char)\n",
    "        store = defaultdict(lambda: 0)\n",
    "        \"\"\"Here we convert all the string\"\"\"\n",
    "        storeminus1 = defaultdict(lambda: 0)\n",
    "        self.V = self.unigrams(file_name)\n",
    "        V = defaultdict(lambda: 0)\n",
    "        with open(file_name) as fp:\n",
    "            line = fp.readline()\n",
    "            #line =\" \"\n",
    "            while line:\n",
    "                line = fp.readline()\n",
    "                line_holder = line.lower()\n",
    "                line_holder = line_holder.translate(trans)\n",
    "                line_holder = line_holder.split('. ')\n",
    "                \"\"\"T is going to be our iterator through the line holder\"\"\"\n",
    "                for T in line_holder:\n",
    "                    \"\"\"This for loop will start to parse the text into sentences\n",
    "                    Check error for strip. \n",
    "                    Getting lower end?\n",
    "                    Not sure check with TA.\"\"\"\n",
    "                    T = T.strip()\n",
    "                    Q = begin_char+T+end_char\n",
    "                    Q = Q.strip()\n",
    "                    lengt = len(Q.split(' '))\n",
    "                    total_amount = self.ngram\n",
    "                    if(total_amount == 1):\n",
    "                        for l in range(1,lengt):\n",
    "                            store[Q.split(' ')[l]] = store[Q.split(' ')[l]]+1\n",
    "                    if(total_amount > 1):\n",
    "                        \"\"\"This will split the words into more sentences.\"\"\"\n",
    "                        for r in range(0,lengt-total_amount+1):\n",
    "                            line_holder = tuple(Q.split(' ')[r:r+(total_amount)])\n",
    "                            store[line_holder] = store[line_holder]+1 \n",
    "                            line_holder = tuple(Q.split(' ')[r:r+(total_amount-1)])\n",
    "                            storeminus1[line_holder] = storeminus1[line_holder]+1\n",
    "        self.model = store\n",
    "        self.modelminus = storeminus1\n",
    "        self.ngramE = self.V ** self.ngram\n",
    "        self.ngramCount = len(store)\n",
    "        if(self.ngram > 1):\n",
    "            self.ngramCount = len(storeminus1)\n",
    "        print(\"Created {} ngrams out of {} potentetial, with a sparsity of: {}\\n\".format(self.ngramCount,self.ngramE,(self.ngramE - self.ngramCount)/self.ngramE))\n",
    "        pass\n",
    "#################################################################################################################\n",
    "    def unigrams(self, file_name):\n",
    "        input_text = \"?:!-\\n\"\n",
    "        output_string  = \"...  \"\n",
    "        \"\"\"Unigrams will be calculated for the entire word bank\n",
    "        \n",
    "        Essentially all the words that are singles are going to be calculated for\n",
    "        the entire unigram\n",
    "        \"\"\"\n",
    "        remove_char  = \",;\\\"(\\)\\\\\\/\"\n",
    "        begin_char = \"<s> \"\n",
    "        end_char = \" </s>\"\n",
    "        trans = str.maketrans(input_text,output_string ,remove_char )\n",
    "        store = defaultdict(lambda: 0)\n",
    "        with open(file_name) as fp:\n",
    "            line = fp.readline()\n",
    "            while line:\n",
    "                line = fp.readline()\n",
    "                line_holder= line.lower()\n",
    "                line_holder= line_holder.translate(trans)\n",
    "                line_holder = line_holder.split('. ')\n",
    "                for T in line_holder:\n",
    "                    T = T.strip()\n",
    "                    Q = begin_char+T+end_char\n",
    "                    Q = Q.strip()\n",
    "                    lengt = len(Q.split(' '))\n",
    "                    total_amount = self.ngram\n",
    "                    for l in range(1,lengt):\n",
    "                        ##Check with TA here??? Broken\n",
    "                        store[Q.split(' ')[l]] = store[Q.split(' ')[l]]+1\n",
    "                \n",
    "        return len(store)\n",
    "#################################################################################################################\n",
    "    def content(self, file):\n",
    "        \"\"\"Content will take the the input string we need to parse out\n",
    "        After w eparse out the unecessary character which include\n",
    "        ? : - \\ m\n",
    "        as well as ...\n",
    "        Then remove all the other slashes.\n",
    "        \n",
    "        Eseentially the content we need to remove.\"\"\"\n",
    "        input_text = \"?:!-\\n\"\n",
    "        output_string  = \"...  \"\n",
    "        remove_char  = \",;\\\"(\\)\\\\\\/\"\n",
    "        trans = input_text.maketrans(input_text ,output_string ,remove_char)\n",
    "        return file.read().lower().translate(trans)\n",
    "#################################################################################################################    \n",
    "    def clean(self, file):\n",
    "        sent = file.split('.')\n",
    "        sentences = []\n",
    "        \"\"\"Clean up file by splitting into sentences\n",
    "        Afer we split up the and it is cleaned into sentences\n",
    "        This is what our clean function will do\n",
    "        \"\"\"\n",
    "        for S in sent:\n",
    "            sentence = ''\n",
    "            for W in S.strip().split(' '):\n",
    "                if re.match(r'\\S',W.strip()):\n",
    "                    sentence += W.strip()+ ' '\n",
    "            sentences.append(\"<s>\" + sentence + \"</s>\")\n",
    "            \n",
    "        return tuple(sentences)\n",
    "#################################################################################################################\n",
    "    def grammer_split(self,sentences,n):\n",
    "        G = []\n",
    "        \"\"\"Grammar split will split it into stences\n",
    "        so one that we can run all our functions on it\n",
    "        If we do not do this. we will get errors.\n",
    "        \"\"\"\n",
    "        for k in sentences:\n",
    "            W = k.split(' ')\n",
    "            for i in range(len(W) - n + 1):\n",
    "                Gn = ''\n",
    "                for j in range(n):\n",
    "                    Gn += W[i+j] + ' '\n",
    "                G.append(Gn.strip())\n",
    "                \n",
    "        return tuple(G)\n",
    "#################################################################################################################\n",
    "    def perplexity(self, ngrams):\n",
    "        N = len(ngrams)\n",
    "        perplexity = 0\n",
    "        \"\"\"This function will calculate the preplexity of the text\n",
    "        \n",
    "        in which then w ewill loop through gram in ngrams\n",
    "        Doing a strip the entire time\"\"\"            \n",
    "        for gram in ngrams:\n",
    "            pro = ''\n",
    "            if self.ngram > 1:\n",
    "                for i in range(0, self.ngram-1):\n",
    "                    pro += gram.split(\" \")[i] + \" \"\n",
    "                    \n",
    "            pro = pro.strip()\n",
    "            countN = 0\n",
    "            if gram in self.model:\n",
    "                countN = self.model[gram]\n",
    "                    \n",
    "            countminusN = 0\n",
    "            if pro in self.modelminus:\n",
    "                countminusN = self.modelminus[pro]\n",
    "            perplexity += log2((countN + 1) / (countminusN + self.V))\n",
    "        perplexity = 2 ** ((-1/N) * perplexity)\n",
    "        return perplexity   \n",
    "#################################################################################################################\n",
    "    def test(self, file_name) :\n",
    "        input_text = \"?:!-\\n\"\n",
    "        output_string  = \"...  \"\n",
    "        remove_char = \",;\\\"(\\)\\\\\\/\"\n",
    "        begin_char = \"<s> \"\n",
    "        end_char= \" </s>\"\n",
    "        \"\"\"Just as same as the other implementations\n",
    "        we have to remove the unecessary character\n",
    "        begining char\n",
    "        end char\n",
    "        and being able to transform it \n",
    "        \n",
    "        Using the input text which will then be outputted to the output_string\"\"\"\n",
    "        trans = str.maketrans(input_text,output_string ,remove_char )\n",
    "        store = defaultdict(lambda: 0)\n",
    "        storeminus1 = defaultdict(lambda: 0)\n",
    "        self.V = self.unigrams(file_name)\n",
    "        V = defaultdict(lambda: 0)\n",
    "        with open(file_name) as file_picker:\n",
    "            line = file_picker.readline()\n",
    "            #line =\" \"\n",
    "            while line:\n",
    "                line = file_picker.readline()\n",
    "                line_holder = line.lower()\n",
    "                line_holder = line_holder.translate(trans)\n",
    "                line_holder = line_holder.split('. ')\n",
    "                for T in line_holder:\n",
    "                    T = T.strip()\n",
    "                    Q = begin_char+T+end_char\n",
    "                    Q = Q.strip()\n",
    "                    lengt = len(Q.split(' '))\n",
    "                    total_amount = self.ngram\n",
    "                    if(total_amount == 1):\n",
    "                        for l in range(1,lengt):\n",
    "                            store[Q.split(' ')[l]] = store[Q.split(' ')[l]]+1\n",
    "\n",
    "                    if(total_amount > 1):\n",
    "                        for r in range(0,lengt-total_amount+1):\n",
    "                            line_holder = tuple(Q.split(' ')[r:r+(total_amount)])\n",
    "                            store[line_holder] = store[line_holder]+1 \n",
    "                            line_holder = tuple(Q.split(' ')[r:r+(total_amount-1)])\n",
    "                            storeminus1[line_holder] = storeminus1[line_holder]+1\n",
    "\n",
    "        self.model = store\n",
    "        self.modelminus = storeminus1\n",
    "        \"\"\"Text is set to self content\n",
    "        OPen file\n",
    "        Conduct grammer commit and then we will be able to text perplexity\n",
    "        IF perplexity values are low then we are doing something incorrect.\n",
    "        \"\"\"\n",
    "        text = self.content(open(file_name,'r'))\n",
    "        sentances = self.clean(text)\n",
    "        test = self.grammer_split(sentances,self.ngram)\n",
    "        perplexity = self.perplexity(test)\n",
    "        return perplexity    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 6464 ngrams out of 6464 potentetial, with a sparsity of: 0.0\n",
      "\n",
      "Created 6464 ngrams out of 41783296 potentetial, with a sparsity of: 0.9998452970297029\n",
      "\n",
      "Created 49649 ngrams out of 270087225344 potentetial, with a sparsity of: 0.999999816174201\n",
      "\n",
      "tests unigram\n",
      "    Bigram\n",
      "    and trigram\n",
      "    using language model.\n",
      "    Total using ngrams as wellUnigram\n",
      "Perplexity of pride_and_prejudice.txt: 30.314312620957555\n",
      "Perplexity of sense_and_sensibility.txt: 30.6189037810618\n",
      "Perplexity of persuasion.txt: 39.12732957122242\n",
      "Perplexity of jane_eyre.txt: 56.890541415807775\n",
      "Bigram\n",
      "Perplexity of pride_and_prejudice.txt: 6463.999999984141\n",
      "Perplexity of sense_and_sensibility.txt: 6488.000000069661\n",
      "Perplexity of persuasion.txt: 5914.999999926937\n",
      "Perplexity of jane_eyre.txt: 12960.000000167925\n",
      "Trigram\n",
      "Perplexity of pride_and_prejudice.txt: 6463.999999998228\n",
      "Perplexity of sense_and_sensibility.txt: 6488.000000054027\n",
      "Perplexity of persuasion.txt: 5914.999999931532\n",
      "Perplexity of jane_eyre.txt: 12960.000000048645\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tests unigram\n",
       "    Bigram\n",
       "    and trigram\n",
       "    using language model.\n",
       "    Total using ngrams as wellUnigram</th>\n",
       "      <th>Bigram</th>\n",
       "      <th>Trigram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pride_and_prejudice.txt</th>\n",
       "      <td>30.314313</td>\n",
       "      <td>6464.0</td>\n",
       "      <td>6464.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sense_and_sensibility.txt</th>\n",
       "      <td>30.618904</td>\n",
       "      <td>6488.0</td>\n",
       "      <td>6488.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>persuasion.txt</th>\n",
       "      <td>39.127330</td>\n",
       "      <td>5915.0</td>\n",
       "      <td>5915.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jane_eyre.txt</th>\n",
       "      <td>56.890541</td>\n",
       "      <td>12960.0</td>\n",
       "      <td>12960.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           tests unigram\\n    Bigram\\n    and trigram\\n    using language model.\\n    Total using ngrams as wellUnigram  \\\n",
       "pride_and_prejudice.txt                                            30.314313                                                              \n",
       "sense_and_sensibility.txt                                          30.618904                                                              \n",
       "persuasion.txt                                                     39.127330                                                              \n",
       "jane_eyre.txt                                                      56.890541                                                              \n",
       "\n",
       "                            Bigram  Trigram  \n",
       "pride_and_prejudice.txt     6464.0   6464.0  \n",
       "sense_and_sensibility.txt   6488.0   6488.0  \n",
       "persuasion.txt              5915.0   5915.0  \n",
       "jane_eyre.txt              12960.0  12960.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tests = {\n",
    "    \"\"\"tests unigram\n",
    "    Bigram\n",
    "    and trigram\n",
    "    using language model.\n",
    "    Total using ngrams as well\"\"\"\n",
    "    \"Unigram\": language_model(1),\n",
    "    \"Bigram\": language_model(2),\n",
    "    \"Trigram\": language_model(3)\n",
    "}\n",
    "books = [\"pride_and_prejudice.txt\",\"sense_and_sensibility.txt\",\"persuasion.txt\",\"jane_eyre.txt\"]\n",
    "preplexingresults = {gram: {} for gram in tests.keys()}\n",
    "\"\"\"\n",
    "Testing for the ngram in which we will call it total_gram\n",
    "\n",
    "loops will be used in test keys\n",
    "we will be collecting all the keys\n",
    "printing out to panda format\"\"\"\n",
    "for total_gram in tests.keys():\n",
    "    tests[total_gram].train(books[0])\n",
    "    for j in range(len(books)):\n",
    "        preplexingresults[total_gram].update({books[j]: tests[total_gram].test(books[j])})\n",
    "for total_gram in tests.keys():\n",
    "    print(total_gram)\n",
    "    for text in preplexingresults[total_gram].keys():\n",
    "        print('Perplexity of {}: {}'.format(text,preplexingresults[total_gram][text]))\n",
    "        \"\"\"\n",
    "        Printing out in panda format here.\"\"\"\n",
    "col = tests.keys()\n",
    "index = books\n",
    "values = preplexingresults\n",
    "pd.DataFrame(preplexingresults,index = index,columns = col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your tasks:\n",
    "\n",
    "Complete the above class for training and evaluating\n",
    "language models using unigrams, bigrams, and trigrams.\n",
    "Before using a text for training or testing:\n",
    "* Convert the text to lower case.\n",
    "* Convert question marks (?), colons (:) and exclamation marks (!) to periods.\n",
    "* Remove all punctuation marks other than the period (commas, semicolons, underscores and quotes); dashes should be converted to spaces.\n",
    "* Parse the text into sentences, adding beginning of sentence and end of sentence tokens.\n",
    "\n",
    "For performing the transformations of the text use Python's string method `str.translate`, which also uses a conversion table that is created by the string method `str.maketrans`.\n",
    "Finally for storing your language model, use a Python dictionary.  You may find it useful to use Python's `defaultdict`, which is part of the `collections` module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Using the code you have written, demonstrate how you would use language models to distinguish between the writings of Jane Austen and some of her contemporaries.  We have provided the text for the following books for you to use in ASCII format:\n",
    "* Pride and Prejudice by Jane Austen\n",
    "* Persuasion by Jane Austen\n",
    "* Sense and Sensibility by Jane Austen\n",
    "* Jane Eyre by Charlotte Bronte\n",
    "* Alice in Wonderland by Lewis Carroll\n",
    "\n",
    "These can be downloaded [here](https://www.cs.colostate.edu/~cs440/fall19/assignments/texts.tar.gz).\n",
    "Original versions of the text are from the [Project Gutenberg](http://www.gutenberg.org/) website. \n",
    "\n",
    "In your analysis, compare the ability of unigram, bigram and trigram models for this task.  Explain your observations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, analyze the models you have constructed for Pride and Prejudice.  How sparse are the bigram and trigram models?  I.e., what fraction of the total number of entries in the model before smoothing are zero?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###ANALYSIS REPORT.\n",
    "\n",
    "Sparsity for sparisty when we had to calculate it for unigram. We didn't see any signicant result, this is due to the fact that the vocabulary and unigram had the same integer value. Overall for sparsity, unigrams did not have a full result to compare and contrast within natrual language processing\n",
    "Bigrams, the sparisty for bigrams always returned a value that was always usually infinitely closer to one\n",
    "sparsity for the trigrams however were always closer to one. this is because the bigram model works better than not only unigram but trigram as well\n",
    "Trigrams has the highest sparsity total, which means its the best model to use out of all three of them. But if the ngram were larger, than time complexity would be much larger which is also a factor to consider\n",
    "    \n",
    "Perplexity shows that we have changes in text as we grow in preplexity, which shows us that the more text we have of the same author and comparing it to other books written by the same author, tends ot have a high preplexity. Calculating the preplexity showed us that unigram had the smallest value, bigrams had the largest preplexity change increase, and the preplexity calculated for trigrams shows that there wasn't a great increase. (Look at data above)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "Answer the questions in the cells reserved for that purpose.\n",
    "Submit your report as a Jupyter notebook via Canvas. Running the notebook should generate all the results in your notebook.  Leave the output of your notebook intact so we don't have to run it to see the results.\n",
    "\n",
    "## Grading\n",
    "\n",
    "Grading will be based on the following criteria:\n",
    "\n",
    "* Your code solves the specified problem.\n",
    "* You performed an analysis that addresses the problem.\n",
    "* Overall readability and organization of the notebook.\n",
    "* Effort in making interesting observations where required.\n",
    "* Conciseness. Points may be taken off if the notebook is overly long."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
